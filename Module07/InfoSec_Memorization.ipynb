{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HsHGPWZBFtS"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHH8AofBE9Ic",
        "outputId": "a0a66b2d-cb55-4382-b5f9-4bcb12778c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  # mount your google drive to get permanent storage for your results\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  RESULTS_PATH = \"/content/drive/MyDrive/infoseclab_ML/results\"\n",
        "except ModuleNotFoundError:\n",
        "  RESULTS_PATH = \"results\"\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkfrTYZ7BHBX",
        "outputId": "223fb58b-2877-4cb1-8277-d236f23dd8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'infoseclab'...\n",
            "remote: Enumerating objects: 321, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 321 (delta 13), reused 31 (delta 10), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (321/321), 64.87 MiB | 14.98 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "/content/infoseclab\n",
            "From https://github.com/ethz-privsec/infoseclab\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Download the lab files\n",
        "![ ! -d 'infoseclab' ] && git clone https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd infoseclab\n",
        "!git pull https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd ..\n",
        "if \"infoseclab\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UFYO94QBIKz"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qN2qQU8dBMG7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import infoseclab\n",
        "from infoseclab import extraction, Vocab, PREFIX\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# we won't need gradients here so let's disable them to make things faster\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# utilities for loading & saving results\n",
        "def read_results():\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"r\") as f:\n",
        "    res = json.load(f)\n",
        "  return res\n",
        "\n",
        "\n",
        "def write_results(res):\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"w\") as f:\n",
        "    res = json.dump(res, f)\n",
        "\n",
        "\n",
        "def print_results(res):\n",
        "  for key, value in res.items():\n",
        "    print(f\"{key.replace('_', ' ')}: {repr(value)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xYlh_fn7ETQ"
      },
      "source": [
        "#Create file to save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmNr00RV7D4z",
        "outputId": "1500d664-8eca-4929-8f36-f9802a949f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  res = read_results()\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "except FileNotFoundError:\n",
        "  res = {\n",
        "      \"main_character\": None,\n",
        "      \"greedy_guess\": None,\n",
        "      \"greedy_numeric_guess\": None,\n",
        "      \"exact_guess\": None\n",
        "  }\n",
        "  write_results(res)\n",
        "\n",
        "print_results(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1PXWtQ-BnGm"
      },
      "source": [
        "#1.&nbsp;Freeform generation\n",
        "\n",
        "We will be working with a simple *character-level* language model.\n",
        "\n",
        "This is a model that takes as input a sentence (e.g., \"my name is \") and outputs a distribution over the next character in the sentence. We can then generate a character (e.g., \"F\") by sampling from this distribution. By applying the model recursively to its own output we can generate text character by character: \"my name is Florian\".\n",
        "\n",
        "Technically, the langauge model doesn't operate on `characters` but on `tokens` (numbers). The characters in the model's \"vocabulary\" are sorted, and can thus be referenced by an integer. The i-th value in the langauge model's output corresponds to the probability assigned to the i-th character in the vocabulary.\n",
        "\n",
        "You can find the full vocabulary (i.e., all characters that the language model can produce) in `infoseclab.extraction.Vocab`.\n",
        "This class has two utility dictionaries, `char_to_ix` and `ix_to_char` for converting from a character to its index (its token) and vice-versa:\n",
        "\n",
        "```\n",
        "Vocab.char_to_ix['a'] -> 54\n",
        "Vocab.ix_to_char[54] -> 'a'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3z_zuKMQ37V1"
      },
      "outputs": [],
      "source": [
        "# load a simple character-level language model\n",
        "lm = extraction.load_lm(\"infoseclab/data/secret_model.pth\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qT7c-NI57MSX",
        "outputId": "4cb66688-1464-4522-c826-4f74c6614582"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world\\n down passion. I cannot lit hear Guvyo. There is a very great dence as I\\n may only clatten his husb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# example of how to generate text from the language model\n",
        "extraction.generate(lm, \"hello world\", length=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKL8VWwn4gVW"
      },
      "source": [
        "**This language model was trained on a collection of texts from a famous British book series. \n",
        "Your first goal is to figure out which books.**\n",
        "\n",
        "**Your guess should be in the form `\"Firstname Lastname\"` of the book series' main character.\n",
        "For example, if you guessed that the book series is Harry Potter, then your guess would be `\"Harry Potter\"`.**\n",
        "\n",
        "Note: the code immediately below doesn't check for correctness! It just checks that you've made a guess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8TT_G_XoQXJ1",
        "outputId": "2dde4709-cc52-4808-9310-5f004a51e293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, my name is Sherl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Own solution\n",
        "extraction.generate(lm, \"Hello, my name is Sher\", length=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXRYQ3B6Bpti",
        "outputId": "85344a3c-2879-4186-e8d0-f0f47a11537e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ],
      "source": [
        "guess = \"Sherlock Holmes\"\n",
        "res = read_results()\n",
        "res['main_character'] = guess\n",
        "write_results(res)\n",
        "print_results(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3c1YfON5bx9"
      },
      "source": [
        "#2.&nbsp;Secret extraction\n",
        "\n",
        "Unfortunately, the training data from this language model also contained the sentence `\"Florian's password is XXXXX\"`. (the real password is blanked out, your goal is to recover it!)\n",
        "\n",
        "The model might have *memorized* the correct password, and your goal will be to recover it.\n",
        "\n",
        "For this, you know the *prefix*: `\"Florian's password is \"`\n",
        "(you can find this stored under `infoseclab.extraction.PREFIX`).\n",
        "\n",
        "You also know that Florian's password is exactly 5 characters long (so that it it easier to memorize, *obviously*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLR_DCUHDt5l"
      },
      "source": [
        "##2.1&nbsp; Greedy secret extraction\n",
        "\n",
        "You will first attempt to extract the secret password *greedily*, simply by sampling the **5 most likely characters**, one-by-one, from the language model, starting from the known `PREFIX`.\n",
        "\n",
        "You can use the `extraction.generate` method as inspiration for this.\n",
        "\n",
        "*Note that `extraction.generate` does <b>not</b> sample greedily from the model. Rather, it samples a character at random according to the probability distribution predicted by the model.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tq-2nO8D0Z9",
        "outputId": "b7d88a02-365c-4dd2-c92b-4686d035b8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy: Florian's password is '3\\n an'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ],
      "source": [
        "def get_next_char(lm, prefix):\n",
        "    \"\"\"calculate most probable next char given the prefix\"\"\"\n",
        "    input_seq = torch.tensor([Vocab.char_to_ix[ch] for ch in prefix]).to(lm.device)\n",
        "\n",
        "    output, _ = lm.forward(input_seq, None)\n",
        "    probs = F.softmax(output[-1], dim=0)\n",
        "\n",
        "    probs_index = dict((j,i) for i,j in enumerate(probs))\n",
        "    sorted_probs = list(sorted(probs_index.items(), reverse=True))\n",
        "    idx = sorted_probs[0][1]\n",
        "    return Vocab.ix_to_char[idx]\n",
        "\n",
        "\n",
        "def generate_greedy(lm, prompt, length=5):\n",
        "    secret = \"\"\n",
        "    for _ in range(length):\n",
        "        next_char = get_next_char(lm, prompt)\n",
        "        secret += next_char\n",
        "        prompt += next_char\n",
        "    \n",
        "    return secret\n",
        "\n",
        "guess_greedy = generate_greedy(lm, PREFIX, length=5)\n",
        "print(\"greedy:\", PREFIX + repr(guess_greedy))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_guess'] = guess_greedy\n",
        "write_results(res)\n",
        "print_results(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCtm2C2L5jep"
      },
      "source": [
        "##2.2&nbsp;Greedy numeric secret extraction\n",
        "\n",
        "Your greedy extraction likely generated some giberish! (but hey, a password might genuinely look like that).\n",
        "\n",
        "You are now given some extra information: **Florian's password only contains numbers!** (he's not very good at security).\n",
        "\n",
        "Modify your greedy sampling mechanism to repeatedly sample the 5 most likely *numbers*, one-by-one, starting from the known `PREFIX`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pbzx4dSa1sy",
        "outputId": "3860c1a6-0eb3-48e8-e5a2-5bb322c03006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy (numeric): Florian's password is '39731'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ],
      "source": [
        "def get_next_digit(lm, prefix):\n",
        "    \"\"\"calculate most probable next digit given the prefix\"\"\"\n",
        "    input_seq = torch.tensor([Vocab.char_to_ix[ch] for ch in prefix]).to(lm.device)\n",
        "\n",
        "    output, _ = lm.forward(input_seq, None)\n",
        "    probs = F.softmax(output[-1], dim=0)\n",
        "\n",
        "    val, idx = max((val, idx) for (idx, val) in enumerate(probs))\n",
        "    probs_index = dict((j,i) for i,j in enumerate(probs))\n",
        "    sorted_probs = list(sorted(probs_index.items(), reverse=True))\n",
        "    \n",
        "    for prob in sorted_probs:\n",
        "        idx = prob[1]\n",
        "        if 12 <= idx and idx <= 21:   # it's a digit ('0' has index 12, '9' has index 21)\n",
        "            return Vocab.ix_to_char[idx]\n",
        "\n",
        "    raise Exception(\"Something went wrong\")\n",
        "\n",
        "def generate_greedy_numeric(lm, prompt, length=5):\n",
        "    secret = \"\"\n",
        "    for _ in range(length):\n",
        "        next_char = get_next_digit(lm, prompt)\n",
        "        secret += next_char\n",
        "        prompt += next_char\n",
        "    \n",
        "    return secret\n",
        "\n",
        "guess_greedy_numeric = generate_greedy_numeric(lm, PREFIX, length=5)\n",
        "print(\"greedy (numeric):\", PREFIX + repr(guess_greedy_numeric))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_numeric_guess'] = guess_greedy_numeric\n",
        "write_results(res)\n",
        "print_results(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16tSQO1RHBxB"
      },
      "source": [
        "##2.3&nbsp;Exact numeric secret extraction\n",
        "\n",
        "Spoiler alert: the secret you found using greedy sampling is *not* Florian's password.\n",
        "\n",
        "As it turns out, sampling greedily from the model is not guaranteed to find the *sequence* of characters that is most likely according to the model's probability distribution.\n",
        "\n",
        "To illustrate, below you can compare the loss from your greedy guess, and a different (also incorrect) guess.</br>\n",
        "The guess `\"36175\"` has lower loss!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5xuvMF7HFg9",
        "outputId": "8978ebe2-a3af-4f66-eb5f-07b16a580a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39731 tensor(0.9791, device='cuda:0')\n",
            "36175 tensor(0.8980, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(guess_greedy_numeric, extraction.get_loss(lm, PREFIX + guess_greedy_numeric))\n",
        "print(\"36175\", extraction.get_loss(lm, PREFIX + \"36175\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkmUuKQWbaVm"
      },
      "source": [
        "Now for the final part, find the 5-digit secret that actually *minimizes* the model's loss, when prompted with the `PREFIX`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjLjFgyTIzgP",
        "outputId": "e9e8e9cd-39d8-42d6-e929-96ae978fe843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "exact: Florian's password is '35192'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ],
      "source": [
        "# Note: runs for around 3 to 5 minutes\n",
        "def generate_exact(lm, prompt, length=5):\n",
        "    min_loss = 999999999999999999\n",
        "    password = \"\"\n",
        "    for num in range(0, 10**length):\n",
        "        guess = f'{num:05}'\n",
        "        loss = extraction.get_loss(lm, prompt + guess)\n",
        "\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss\n",
        "            password = guess\n",
        "\n",
        "    return password\n",
        "\n",
        "\n",
        "guess_exact = generate_exact(lm, PREFIX, length=5)\n",
        "print(\"\\nexact:\", PREFIX + repr(guess_exact))\n",
        "\n",
        "res = read_results()\n",
        "res['exact_guess'] = guess_exact\n",
        "write_results(res)\n",
        "print_results(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNMIfOoL_dOt"
      },
      "source": [
        "# Create submission file (**upload `results.zip` to moodle**) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0N1Uv1Y_cLk",
        "outputId": "ecbf45d6-99bb-49f8-c5f0-af9eef45da7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: extraction.json (deflated 25%)\n",
            "updating: x_adv_targeted.npy (deflated 10%)\n",
            "updating: x_adv_detect.npy (deflated 10%)\n",
            "updating: x_adv_random.npy (deflated 10%)\n"
          ]
        }
      ],
      "source": [
        "!zip -j -r \"{RESULTS_PATH}/results.zip\" {RESULTS_PATH} --exclude \"*x_adv_untargeted.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSPUajuP_zcd",
        "outputId": "af6f3466-735b-44ac-f1b4-8139a9d7a0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ],
      "source": [
        "with ZipFile(f\"{RESULTS_PATH}/results.zip\", 'r') as zip:\n",
        "    res = json.load(zip.open(\"extraction.json\"))\n",
        "    print_results(res)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "infsec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "828f02381a8e0f56f5915c0b0ff4bafd5c68cdd3244e1ed49e9ce3b8f035d265"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}